{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling Efforts.\n",
    "\n",
    "For this project, I had to gather, assess and clean three datasets with tweet information\n",
    "from the twitter account @dog_rates, also known as We Rate Dogs.\n",
    "\n",
    "I first directly downloaded the 'twitter-archive-enhanced.csv’ dataset.\n",
    "I then used the requests library to programmatically download the ‘image_predictions.tsv’\n",
    "dataset. I tried to use the tweepy library to scrap data through api keys I got from\n",
    "twitter, but I could not get elevated access via the developer portal. Therefore, I \n",
    "got the data I would have gotten scraping from a link Udacity provided. The link provided a \n",
    "file called tweet_json.txt and I opened it using the read_json function.\n",
    "\n",
    "I first started working on tidiness issues. The first task was to join the dataframes made from the 'twitter-archive-enhanced.csv’, ‘image_predictions.tsv’ and ‘tweet_json.txt’ datasets. I joined the dataframes on the columns with tweet id data. I had to left join ‘image_predictions.tsv’ and ‘tweet_json.txt’ into 'twitter-archive-enhanced.csv’. Also, the dog stage information was in 4 different columns. I had to consolidate the 4 different columns into one column called ‘dog_stages’. I had to make a function and a for loop in order to clean the data after consolidating the columns.\n",
    "\n",
    "I then worked on quality issues with the dataframe. I first converted columns with id data that were either integers or floats into strings. I used the astype function along with lambda functions to do this task.\n",
    "\n",
    "In the “name” column, there were 55 rows which had the value “a”. I checked one of the urls\n",
    "and while the tweet was a rating of a dog picture, the dog’s name was not “a”. It seemed that\n",
    "the dog’s name was mistakenly filled as ‘a’. I replace the ’a’ values with ‘None’ using the\n",
    "replace function in the “name column.\n",
    "\n",
    "Since only original ratings (no retweets) that have images were needed in the dataset,  I had to delete rows where \"in_reply_to_user_id”, “in_reply_to_status_id”, \"retweeted_status_id\" and \"retweeted_status_user_id”. I also had to delete the row of index 315 as it was a retweet of another twitter account plagiarizing We Rate Dogs. Rows with duplicated jpg urls were deleted as well as we wanted ratings of distinct images. Finally, index 35 was deleted as it did not have a  jpg url and the expanded url was not a tweet from the We Rate Dogs twitter account.\n",
    "\n",
    "After all these steps, I saved the dataframe as a csv file named \"twitter_archive_master.csv\" with the to_csv function. The data was ready to be analyzed and visualized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
